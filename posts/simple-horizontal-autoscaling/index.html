<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Horizontal Pod Autoscaler in Kubernetes (Part 1) — Simple Autoscaling using Metrics Server</title>
    <link rel="stylesheet" href="/css/bulma.min.css">
    <link rel="stylesheet" href="/css/custom.css">
    <link rel="stylesheet" href="/css/hugo.css">
</head>

<body>
<nav class="navbar">
    <div class="navbar-brand">
        <a class="navbar-item" href="/">
            Home
        </a>

        <div class="navbar-item">
            <a class="button is-link" href="/about">
                <strong>About me</strong>
            </a>
        </div>
    </div>
</nav>

<section class="section">
    <div class="container is-max-desktop">
        <div class="content">
            <p class="title is-spaced">Horizontal Pod Autoscaler in Kubernetes (Part 1) — Simple Autoscaling using Metrics Server</p>
            
            
            <div class="box">
                <p class="subtitle">Table of Contents</p>
                <nav id="TableOfContents">
  <ul>
    <li><a href="#requirements">Requirements</a></li>
    <li><a href="#creating-kind-cluster">Creating Kind Cluster</a></li>
    <li><a href="#installing-nginx-ingress-controller">Installing NGINX Ingress Controller</a></li>
    <li><a href="#installing-metrics-server">Installing Metrics Server</a></li>
    <li><a href="#horizontal-pod-autoscaler">Horizontal Pod Autoscaler</a></li>
    <li><a href="#autoscaling-native-services-based-on-cpu-and-memory-usage">Autoscaling Native Services based on CPU and Memory Usage</a></li>
    <li><a href="#autoscaling-jvm-services-based-on-cpu-usage">Autoscaling JVM Services based on CPU Usage</a></li>
    <li><a href="#wrapping-up">Wrapping up</a></li>
  </ul>
</nav>
            </div>
            
            
        </div>
    </div>
    <div class="container is-max-desktop mt-6">
        <div class="content has-text-justified-tablet">
            <p>The Horizontal Pod Autoscaler (HPA) is a fundamental feature of Kubernetes. It enables automatic scale-up and scale-down of containerized applications based on CPU usage, memory usage, or custom metrics.</p>
<p>Traditionally, when scaling software, we first think of vertical scaling: the CPU and the RAM are increased so the application consuming them can perform better. While this seems like a flawless mechanism on paper, it actually comes with many drawbacks.</p>
<p>First, upgrading the CPU or RAM on a physical machine (or VM) requires downtime and unless a Pod Disruption Budget (PDB) is used to handle <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions">disruptions</a>, all pods will be evicted and recreated in the new resized node.</p>
<p>Nodes resource usage is also not optimized, as scaling vertically means requiring sufficient resources in a single node, while horizontal scaling may have the same amount of resources distributed across multiple nodes.</p>
<p>Additionally, vertical scaling is not as resilient as horizontal scaling, as fewer replicas mean higher risks of disruptions in case of node failure.</p>
<p>Finally, reaching a certain threshold, scaling only vertically becomes very expensive and most importantly, isn’t limitless. In fact, there is only so much CPU and RAM a physical machine(or VM) alone can handle.<br>
This is where horizontal scaling comes into play!</p>
<blockquote>
<p>Eventually, it is more efficient to duplicate an instance, than increase its resources.</p>
</blockquote>
<p>🎬 Hi there, I&rsquo;m Jean!</p>
<p>In this 2 parts series, we&rsquo;re going to explore several ways to scale services horizontally in Kubernetes, and the first one is…<br>
🥁<br>
… using <strong>Metrics Server</strong>! 🎊</p>
<h2 id="requirements">Requirements</h2>
<hr>
<p>Before we start, make sure you have the following tools installed:</p>
<ul>
<li><a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installation">Kind</a></li>
<li><a href="https://kubernetes.io/docs/tasks/tools/">Kubectl</a></li>
<li><a href="https://helm.sh/docs/intro/install/">Helm</a></li>
<li><a href="https://k6.io/docs/getting-started/installation/">K6</a></li>
</ul>
<blockquote>
<p><em>Note: for MacOS users or Linux users using Homebrew, simply run:</em><br>
<code>brew install kind kubectl helm k6</code></p>
</blockquote>
<p>All set? Let’s go! 🏁</p>
<h2 id="creating-kind-cluster">Creating Kind Cluster</h2>
<hr>
<p><a href="https://kind.sigs.k8s.io/">Kind</a> is a tool for running local Kubernetes clusters using Docker container “nodes”.
It was primarily designed for testing Kubernetes itself, but may be used for local development or CI.</p>
<p>I don’t expect you to have a demo project in handy, so <a href="https://github.com/jhandguy/canary-deployment">I built one</a> for you.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">git clone https://github.com/jhandguy/horizontal-pod-autoscaler.git
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> horizontal-pod-autoscaler
</span></span></code></pre></div><p>Alright, let&rsquo;s spin up our Kind cluster! 🚀</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">➜ kind create cluster --image kindest/node:v1.27.3 --config<span class="o">=</span>kind/cluster.yaml
</span></span><span class="line"><span class="cl">Creating cluster <span class="s2">&#34;kind&#34;</span> ...
</span></span><span class="line"><span class="cl"> ✓ Ensuring node image <span class="o">(</span>kindest/node:v1.27.3<span class="o">)</span> 🖼
</span></span><span class="line"><span class="cl"> ✓ Preparing nodes 📦
</span></span><span class="line"><span class="cl"> ✓ Writing configuration 📜
</span></span><span class="line"><span class="cl"> ✓ Starting control-plane 🕹️
</span></span><span class="line"><span class="cl"> ✓ Installing CNI 🔌
</span></span><span class="line"><span class="cl"> ✓ Installing StorageClass 💾
</span></span><span class="line"><span class="cl">Set kubectl context to <span class="s2">&#34;kind-kind&#34;</span>
</span></span><span class="line"><span class="cl">You can now use your cluster with:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl cluster-info --context kind-kind
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community 🙂
</span></span></code></pre></div><h2 id="installing-nginx-ingress-controller">Installing NGINX Ingress Controller</h2>
<hr>
<p><a href="https://github.com/kubernetes/ingress-nginx">NGINX Ingress Controller</a> is one of the <a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers">many available Kubernetes Ingress Controllers</a>, which acts as a load balancer and satisfies routing rules specified in <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#what-is-ingress">Ingress</a> resources, using the <a href="https://nginx.org/en/">NGINX reverse proxy</a>.</p>
<p>NGINX Ingress Controller can be installed via its <a href="https://github.com/kubernetes/ingress-nginx/tree/main/charts/ingress-nginx">Helm chart</a>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
</span></span><span class="line"><span class="cl">helm install ingress-nginx/ingress-nginx --name-template ingress-nginx --create-namespace -n ingress-nginx --values kind/ingress-nginx-values.yaml --version 4.8.3 --wait
</span></span></code></pre></div><p>Now, if everything goes according to plan, you should be able to see the <strong>ingress-nginx-controller</strong> Deployment running.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">➜ kubectl get deploy -n ingress-nginx
</span></span><span class="line"><span class="cl">NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span class="line"><span class="cl">ingress-nginx-controller   1/1     <span class="m">1</span>            <span class="m">1</span>           4m35s
</span></span></code></pre></div><h2 id="installing-metrics-server">Installing Metrics Server</h2>
<hr>
<p><a href="https://github.com/kubernetes-sigs/metrics-server">Metrics Server</a> is a source of container resource metrics, which collects them from Kubelets and exposes them in Kubernetes API server through <a href="https://github.com/kubernetes/metrics">Metrics API</a> for use by <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler</a> and <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler/">Vertical Pod Autoscaler</a>.</p>
<p>Metrics Server can be installed via its <a href="https://github.com/kubernetes-sigs/metrics-server/tree/master/charts/metrics-server">Helm chart</a>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server
</span></span><span class="line"><span class="cl">helm install metrics-server/metrics-server --name-template metrics-server --create-namespace -n metrics-server --values kind/metrics-server-values.yaml --version 3.11.0 --wait
</span></span></code></pre></div><p>Now, if everything goes according to plan, you should be able to see the <strong>metrics-server</strong> Deployment running.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">➜ kubectl get deploy -n metrics-server
</span></span><span class="line"><span class="cl">NAME             READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span class="line"><span class="cl">metrics-server   1/1     <span class="m">1</span>            <span class="m">1</span>           38s
</span></span></code></pre></div><h2 id="horizontal-pod-autoscaler">Horizontal Pod Autoscaler</h2>
<hr>
<p>Before we dive in, let&rsquo;s quickly remind ourselves of what a <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">Horizontal Pod Autoscaler</a> in Kubernetes actually is:</p>
<blockquote>
<p><em>A <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">HorizontalPodAutoscaler</a> (HPA for short) automatically updates a workload resource (such as a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a> or <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>), with the aim of automatically scaling the workload to match demand.</em><br>
<em>Horizontal scaling means that the response to increased load is to deploy more <a href="https://kubernetes.io/docs/concepts/workloads/pods/">Pods</a>. This is different from vertical scaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU) to the Pods that are already running for the workload.</em><br>
<em>If the load decreases, and the number of Pods is above the configured minimum, the HorizontalPodAutoscaler instructs the workload resource (the Deployment, StatefulSet, or other similar resource) to scale back down.</em></p>
</blockquote>
<p>Now that we know what an HPA is, let&rsquo;s get started, shall we? 🧐</p>
<h2 id="autoscaling-native-services-based-on-cpu-and-memory-usage">Autoscaling Native Services based on CPU and Memory Usage</h2>
<hr>
<p>A native service is a piece  of software that does not require a virtual environment in order to run across different OSs and CPU architectures. This is the case  for C/C++, Golang, Rust, and more: those are languages that compile into a binary, that is directly executable by the Pod.<br>
This means that native services can utilize all of the CPU and memory  available from the Pod they run in, without an intermediary environment.</p>
<p>Let&rsquo;s try it out with a Golang service!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">helm install golang-sample-app/helm-chart --name-template sample-app --create-namespace -n sample-app --wait
</span></span></code></pre></div><p>If everything goes fine, you should eventually see one Deployment with the READY state.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">➜ kubectl get deploy -n sample-app
</span></span><span class="line"><span class="cl">NAME         READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span class="line"><span class="cl">sample-app   2/2     <span class="m">2</span>            <span class="m">2</span>           44s
</span></span></code></pre></div><p>Once the Pods are running, Metrics Server will start collecting the Pods resource metrics from the node&rsquo;s Kubelet and expose them in Kubernetes through <a href="https://github.com/kubernetes/metrics">Metrics API</a> for use by <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler</a> and <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler/">Vertical Pod Autoscaler</a>.</p>
<p>Let&rsquo;s see what the resource usage for those Pods currently is!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">➜ kubectl top pods -n sample-app
</span></span><span class="line"><span class="cl">NAME                          CPU<span class="o">(</span>cores<span class="o">)</span>   MEMORY<span class="o">(</span>bytes<span class="o">)</span>
</span></span><span class="line"><span class="cl">sample-app-6bcbfc8b49-j6xmq   1m           1Mi
</span></span><span class="line"><span class="cl">sample-app-6bcbfc8b49-wtd8g   1m           1Mi
</span></span></code></pre></div><p>Pretty low, right? 🤔<br>
This is obviously expected since our Go service currently isn&rsquo;t handling any load.</p>
<p>Alright, now let&rsquo;s have a look at the HPA!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">➜ kubectl describe hpa -n sample-app
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">Metrics: <span class="o">(</span> current / target <span class="o">)</span>
</span></span><span class="line"><span class="cl">  resource memory on pods <span class="o">(</span>as a percentage of request<span class="o">)</span>:  8% / 50%
</span></span><span class="line"><span class="cl">  resource cpu on pods <span class="o">(</span>as a percentage of request<span class="o">)</span>:     10% / 50%
</span></span><span class="line"><span class="cl">Min replicas:                                            <span class="m">2</span>
</span></span><span class="line"><span class="cl">Max replicas:                                            <span class="m">8</span>
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>As you can see, this HPA is configured to scale the service based on both CPU and memory, with average utilization of <strong>50%</strong> each.</p>
<p>This means that as soon as either the CPU or memory utilization breaches the <strong>50%</strong> threshold, the HPA will trigger an upscale.</p>
<p>Under minimal load, the HPA will still retain a replica count of <strong>2</strong>, while the maximum amount of Pods the HPA is allowed to spin up under high load is <strong>8</strong>.</p>
<blockquote>
<p><em>Note: in a production environment, it is recommended to have a minimum replica count of at least 3, to guarantee maintained availability in the case of <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/_print/#pod-disruption">Pod Disruption</a>.</em></p>
</blockquote>
<p>Now, this is the moment you&rsquo;ve certainly expected… It&rsquo;s Load Testing time! 😎</p>
<p>For Load Testing, I really recommend <a href="https://k6.io/docs/">k6</a> from the Grafana Labs team. It is a dead-simple yet super powerful tool with very extensive documentation.</p>
<p>See for yourself!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">k6 run k6/script.js
</span></span></code></pre></div><p>While the load test is running, I suggest watching the HPA in a separate tab.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl get hpa -n sample-app -w
</span></span></code></pre></div><p>As the load test progresses and the 2 starting Pods struggle to handle incoming requests, you should see both CPU and memory targets increasing, and ultimately, the <strong>replica count reaching its maximum</strong>!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">Deployment/sample-app   16%/50%, 10%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">2</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   16%/50%, 15%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">2</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   17%/50%, 40%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">2</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   18%/50%, 50%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">2</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   19%/50%, 60%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">2</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   22%/50%, 75%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">3</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   27%/50%, 85%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">3</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   24%/50%, 80%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">4</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   27%/50%, 80%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">5</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   22%/50%, 72%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">5</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   23%/50%, 70%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">6</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   25%/50%, 64%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">7</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   24%/50%, 61%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">7</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   25%/50%, 61%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">7</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   27%/50%, 60%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">8</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   28%/50%, 60%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">8</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   27%/50%, 57%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">8</span>
</span></span></code></pre></div><p>When relying on multiple targets for a single HPA, you can find out which of those have triggered the up/downscale by consulting Kubernetes events.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">➜ kubectl get events -n sample-app
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">New size: 3<span class="p">;</span> reason: cpu resource utilization <span class="o">(</span>percentage of request<span class="o">)</span> above target
</span></span><span class="line"><span class="cl">Scaled up replica <span class="nb">set</span> sample-app-6bcbfc8b49 to <span class="m">3</span>
</span></span><span class="line"><span class="cl">New size: 4<span class="p">;</span> reason: cpu resource utilization <span class="o">(</span>percentage of request<span class="o">)</span> above target
</span></span><span class="line"><span class="cl">Scaled up replica <span class="nb">set</span> sample-app-6bcbfc8b49 to <span class="m">4</span>
</span></span><span class="line"><span class="cl">New size: 5<span class="p">;</span> reason: cpu resource utilization <span class="o">(</span>percentage of request<span class="o">)</span> above target
</span></span><span class="line"><span class="cl">Scaled up replica <span class="nb">set</span> sample-app-6bcbfc8b49 to <span class="m">5</span>
</span></span><span class="line"><span class="cl">New size: 6<span class="p">;</span> reason: cpu resource utilization <span class="o">(</span>percentage of request<span class="o">)</span> above target
</span></span><span class="line"><span class="cl">Scaled up replica <span class="nb">set</span> sample-app-6bcbfc8b49 to <span class="m">6</span>
</span></span><span class="line"><span class="cl">New size: 7<span class="p">;</span> reason: cpu resource utilization <span class="o">(</span>percentage of request<span class="o">)</span> above target
</span></span><span class="line"><span class="cl">Scaled up replica <span class="nb">set</span> sample-app-6bcbfc8b49 to <span class="m">7</span>
</span></span><span class="line"><span class="cl">New size: 8<span class="p">;</span> reason: cpu resource utilization <span class="o">(</span>percentage of request<span class="o">)</span> above target
</span></span><span class="line"><span class="cl">Scaled up replica <span class="nb">set</span> sample-app-6bcbfc8b49 to <span class="m">8</span>
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>Now, let&rsquo;s quickly have a look at the Load Test summary and the result of the <code>http_req_duration</code> metric in particular!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">          /<span class="se">\ </span>     <span class="p">|</span>‾‾<span class="p">|</span> /‾‾/   /‾‾/
</span></span><span class="line"><span class="cl">     /<span class="se">\ </span> /  <span class="se">\ </span>    <span class="p">|</span>  <span class="p">|</span>/  /   /  /
</span></span><span class="line"><span class="cl">    /  <span class="se">\/</span>    <span class="se">\ </span>   <span class="p">|</span>     <span class="o">(</span>   /   ‾‾<span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   /          <span class="se">\ </span>  <span class="p">|</span>  <span class="p">|</span><span class="se">\ </span> <span class="se">\ </span><span class="p">|</span>  <span class="o">(</span>‾<span class="o">)</span>  <span class="p">|</span>
</span></span><span class="line"><span class="cl">  / __________ <span class="se">\ </span> <span class="p">|</span>__<span class="p">|</span> <span class="se">\_</span>_<span class="se">\ \_</span>____/ .io
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  execution: <span class="nb">local</span>
</span></span><span class="line"><span class="cl">     script: k6/script.js
</span></span><span class="line"><span class="cl">     output: -
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  scenarios: <span class="o">(</span>100.00%<span class="o">)</span> <span class="m">1</span> scenario, <span class="m">100</span> max VUs, 5m30s max duration <span class="o">(</span>incl. graceful stop<span class="o">)</span>:
</span></span><span class="line"><span class="cl">           * load: Up to 100.00 iterations/s <span class="k">for</span> 5m0s over <span class="m">2</span> stages <span class="o">(</span>maxVUs: 100, gracefulStop: 30s<span class="o">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">     ✓ status code is <span class="m">200</span>
</span></span><span class="line"><span class="cl">     ✓ node is kind-control-plane
</span></span><span class="line"><span class="cl">     ✓ namespace is sample-app
</span></span><span class="line"><span class="cl">     ✓ pod is sample-app-*
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   ✓ checks.........................: 100.00% ✓ <span class="m">60360</span>    ✗ <span class="m">0</span>
</span></span><span class="line"><span class="cl">     data_received..................: 3.5 MB  <span class="m">12</span> kB/s
</span></span><span class="line"><span class="cl">     data_sent......................: 1.7 MB  5.8 kB/s
</span></span><span class="line"><span class="cl">     http_req_blocked...............: <span class="nv">avg</span><span class="o">=</span>17.57µs  <span class="nv">min</span><span class="o">=</span>1µs      <span class="nv">med</span><span class="o">=</span>9µs    <span class="nv">max</span><span class="o">=</span>5.48ms  p<span class="o">(</span>90<span class="o">)=</span>17µs    p<span class="o">(</span>95<span class="o">)=</span>21µs
</span></span><span class="line"><span class="cl">     http_req_connecting............: <span class="nv">avg</span><span class="o">=</span>4.36µs   <span class="nv">min</span><span class="o">=</span>0s       <span class="nv">med</span><span class="o">=</span>0s     <span class="nv">max</span><span class="o">=</span>5.26ms  p<span class="o">(</span>90<span class="o">)=</span>0s      p<span class="o">(</span>95<span class="o">)=</span>0s
</span></span><span class="line"><span class="cl">   ✓ http_req_duration..............: <span class="nv">avg</span><span class="o">=</span>17.63ms  <span class="nv">min</span><span class="o">=</span>496µs    <span class="nv">med</span><span class="o">=</span>3.16ms <span class="nv">max</span><span class="o">=</span>1.76s   p<span class="o">(</span>90<span class="o">)=</span>11.38ms p<span class="o">(</span>95<span class="o">)=</span>51.18ms
</span></span><span class="line"><span class="cl">       <span class="o">{</span> expected_response:true <span class="o">}</span>...: <span class="nv">avg</span><span class="o">=</span>17.63ms  <span class="nv">min</span><span class="o">=</span>496µs    <span class="nv">med</span><span class="o">=</span>3.16ms <span class="nv">max</span><span class="o">=</span>1.76s   p<span class="o">(</span>90<span class="o">)=</span>11.38ms p<span class="o">(</span>95<span class="o">)=</span>51.18ms
</span></span><span class="line"><span class="cl">     http_req_failed................: 0.00%   ✓ <span class="m">0</span>        ✗ <span class="m">15090</span>
</span></span><span class="line"><span class="cl">     http_req_receiving.............: <span class="nv">avg</span><span class="o">=</span>107.72µs <span class="nv">min</span><span class="o">=</span>10µs     <span class="nv">med</span><span class="o">=</span>78µs   <span class="nv">max</span><span class="o">=</span>7.62ms  p<span class="o">(</span>90<span class="o">)=</span>164µs   p<span class="o">(</span>95<span class="o">)=</span>214µs
</span></span><span class="line"><span class="cl">     http_req_sending...............: <span class="nv">avg</span><span class="o">=</span>53.87µs  <span class="nv">min</span><span class="o">=</span>5µs      <span class="nv">med</span><span class="o">=</span>35µs   <span class="nv">max</span><span class="o">=</span>15.33ms p<span class="o">(</span>90<span class="o">)=</span>73µs    p<span class="o">(</span>95<span class="o">)=</span>95µs
</span></span><span class="line"><span class="cl">     http_req_tls_handshaking.......: <span class="nv">avg</span><span class="o">=</span>0s       <span class="nv">min</span><span class="o">=</span>0s       <span class="nv">med</span><span class="o">=</span>0s     <span class="nv">max</span><span class="o">=</span>0s      p<span class="o">(</span>90<span class="o">)=</span>0s      p<span class="o">(</span>95<span class="o">)=</span>0s
</span></span><span class="line"><span class="cl">     http_req_waiting...............: <span class="nv">avg</span><span class="o">=</span>17.47ms  <span class="nv">min</span><span class="o">=</span>423µs    <span class="nv">med</span><span class="o">=</span>2.99ms <span class="nv">max</span><span class="o">=</span>1.76s   p<span class="o">(</span>90<span class="o">)=</span>11.08ms p<span class="o">(</span>95<span class="o">)=</span>51.02ms
</span></span><span class="line"><span class="cl">     http_reqs......................: <span class="m">15090</span>   50.29863/s
</span></span><span class="line"><span class="cl">     iteration_duration.............: <span class="nv">avg</span><span class="o">=</span>18.11ms  <span class="nv">min</span><span class="o">=</span>626.66µs <span class="nv">med</span><span class="o">=</span>3.64ms <span class="nv">max</span><span class="o">=</span>1.76s   p<span class="o">(</span>90<span class="o">)=</span>12.03ms p<span class="o">(</span>95<span class="o">)=</span>51.78ms
</span></span><span class="line"><span class="cl">     iterations.....................: <span class="m">15090</span>   50.29863/s
</span></span><span class="line"><span class="cl">     vus............................: <span class="m">0</span>       <span class="nv">min</span><span class="o">=</span><span class="m">0</span>      <span class="nv">max</span><span class="o">=</span><span class="m">18</span>
</span></span><span class="line"><span class="cl">     vus_max........................: <span class="m">100</span>     <span class="nv">min</span><span class="o">=</span><span class="m">100</span>    <span class="nv">max</span><span class="o">=</span><span class="m">100</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">running <span class="o">(</span>5m00.0s<span class="o">)</span>, 000/100 VUs, <span class="m">15090</span> <span class="nb">complete</span> and <span class="m">0</span> interrupted iterations
</span></span><span class="line"><span class="cl">load ✓ <span class="o">[======================================]</span> 000/100 VUs  5m0s  000.65 iters/s
</span></span></code></pre></div><p>As you can observe, our Golang service has performed very well under heavy load, with a Success Share of 100%, a median latency of ~3ms, and a 95th percentile latency of ~50ms!</p>
<p>We have the HPA to thank for that, as it scaled the Deployment from 2 to 8 Pods swiftly and automatically, based on the Pods resource usage!</p>
<p>We definitely would not have had the same results without an HPA… Actually, why don&rsquo;t you try it yourself? 😉</p>
<p>Just delete the HPA (<code>kubectl delete hpa sample-app -n sample-app</code>), run the load test again (<code>k6 run k6/script.js</code>) and see what happens! (spoiler alert: it&rsquo;s not pretty 😬)</p>
<p>Once you are done, don&rsquo;t forget to uninstall the Helm release! (we won&rsquo;t be needing this one anymore)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">helm uninstall sample-app -n sample-app
</span></span></code></pre></div><h2 id="autoscaling-jvm-services-based-on-cpu-usage">Autoscaling JVM Services based on CPU Usage</h2>
<hr>
<p>While native services run as executable binaries, JVM services need an extra environment layer in order to run on various OSs and CPU architectures: the Java Virtual Machine (JVM).</p>
<p>This creates an issue for Pod Autoscaling, as the JVM pre-allocates more memory than it actually needs from the Pod&rsquo;s resources, for its Garbage Collector (GC). This makes memory altogether an unreliable metric to use for autoscaling a JVM-based service in Kubernetes via Metrics Server.</p>
<p>Thus, in the case of Java or other JVM-based  services, when utilizing Metrics Server for HPA, one can only rely on the CPU metric for autoscaling.</p>
<p>Let&rsquo;s experience it with a Kotlin/JVM service!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">helm install kotlin-sample-app/helm-chart --name-template sample-app --create-namespace -n sample-app --wait
</span></span></code></pre></div><p>If everything goes fine, you should eventually see one Deployment with the READY state.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">➜ kubectl get deploy -n sample-app
</span></span><span class="line"><span class="cl">NAME         READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span class="line"><span class="cl">sample-app   2/2     <span class="m">2</span>            <span class="m">2</span>           52s
</span></span></code></pre></div><p>Let&rsquo;s see what the resource usage for those Pods running a JVM currently is!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">➜ kubectl top pods -n sample-app
</span></span><span class="line"><span class="cl">NAME                         CPU<span class="o">(</span>cores<span class="o">)</span>   MEMORY<span class="o">(</span>bytes<span class="o">)</span>
</span></span><span class="line"><span class="cl">sample-app-8df8cfcd4-lg9s8   7m           105Mi
</span></span><span class="line"><span class="cl">sample-app-8df8cfcd4-r9fjh   7m           105Mi
</span></span></code></pre></div><p>Interesting! As you can see, while being idle, both pods consume ~100Mi (~104Mb) of memory, which is almost 50% of the Pods memory limit! 😱<br>
As previously stated, this is due to the JVM pre-allocating memory for its Garbage Collector (GC).</p>
<p>Alright, now let&rsquo;s have a look at the HPA!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">➜ kubectl describe hpa -n sample-app
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">Metrics: <span class="o">(</span> current / target <span class="o">)</span>
</span></span><span class="line"><span class="cl">  resource cpu on pods <span class="o">(</span>as a percentage of request<span class="o">)</span>:  10% / 50%
</span></span><span class="line"><span class="cl">Min replicas:                                         <span class="m">2</span>
</span></span><span class="line"><span class="cl">Max replicas:                                         <span class="m">8</span>
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>As announced, this time the HPA only relies on one resource metric: the CPU.</p>
<p>Alright, let&rsquo;s give our favorite Load Testing tool another go! 🚀</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">k6 run k6/script.js
</span></span></code></pre></div><p>As previously mentioned, I suggest watching the HPA in a separate tab.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl get hpa -n sample-app -w
</span></span></code></pre></div><p>As the load test progresses and the 2 starting Pods struggle to handle incoming requests, you should see the CPU target increasing, and ultimately, the <strong>replica count reaching its maximum</strong>!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">Deployment/sample-app   10%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">2</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   15%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">2</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   36%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">2</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   64%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">2</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   37%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">3</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   41%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">3</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   51%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">3</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   99%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">3</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   56%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">6</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   50%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">6</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   76%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">6</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   74%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">8</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   61%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">8</span>
</span></span><span class="line"><span class="cl">Deployment/sample-app   58%/50%   <span class="m">2</span>         <span class="m">8</span>         <span class="m">8</span>
</span></span></code></pre></div><p>Now, let&rsquo;s quickly have a look at the Load Test summary and the result of the <code>http_req_duration</code> metric in particular!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">          /<span class="se">\ </span>     <span class="p">|</span>‾‾<span class="p">|</span> /‾‾/   /‾‾/
</span></span><span class="line"><span class="cl">     /<span class="se">\ </span> /  <span class="se">\ </span>    <span class="p">|</span>  <span class="p">|</span>/  /   /  /
</span></span><span class="line"><span class="cl">    /  <span class="se">\/</span>    <span class="se">\ </span>   <span class="p">|</span>     <span class="o">(</span>   /   ‾‾<span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>   /          <span class="se">\ </span>  <span class="p">|</span>  <span class="p">|</span><span class="se">\ </span> <span class="se">\ </span><span class="p">|</span>  <span class="o">(</span>‾<span class="o">)</span>  <span class="p">|</span>
</span></span><span class="line"><span class="cl">  / __________ <span class="se">\ </span> <span class="p">|</span>__<span class="p">|</span> <span class="se">\_</span>_<span class="se">\ \_</span>____/ .io
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  execution: <span class="nb">local</span>
</span></span><span class="line"><span class="cl">     script: k6/script.js
</span></span><span class="line"><span class="cl">     output: -
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  scenarios: <span class="o">(</span>100.00%<span class="o">)</span> <span class="m">1</span> scenario, <span class="m">100</span> max VUs, 5m30s max duration <span class="o">(</span>incl. graceful stop<span class="o">)</span>:
</span></span><span class="line"><span class="cl">           * load: Up to 100.00 iterations/s <span class="k">for</span> 5m0s over <span class="m">2</span> stages <span class="o">(</span>maxVUs: 100, gracefulStop: 30s<span class="o">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">     ✓ status code is <span class="m">200</span>
</span></span><span class="line"><span class="cl">     ✓ node is kind-control-plane
</span></span><span class="line"><span class="cl">     ✓ namespace is sample-app
</span></span><span class="line"><span class="cl">     ✓ pod is sample-app-*
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   ✓ checks.........................: 100.00% ✓ <span class="m">60360</span>     ✗ <span class="m">0</span>
</span></span><span class="line"><span class="cl">     data_received..................: 3.3 MB  <span class="m">11</span> kB/s
</span></span><span class="line"><span class="cl">     data_sent......................: 1.7 MB  5.8 kB/s
</span></span><span class="line"><span class="cl">     http_req_blocked...............: <span class="nv">avg</span><span class="o">=</span>18.56µs  <span class="nv">min</span><span class="o">=</span>1µs    <span class="nv">med</span><span class="o">=</span>9µs    <span class="nv">max</span><span class="o">=</span>2.37ms p<span class="o">(</span>90<span class="o">)=</span>17µs    p<span class="o">(</span>95<span class="o">)=</span>20µs
</span></span><span class="line"><span class="cl">     http_req_connecting............: <span class="nv">avg</span><span class="o">=</span>5.52µs   <span class="nv">min</span><span class="o">=</span>0s     <span class="nv">med</span><span class="o">=</span>0s     <span class="nv">max</span><span class="o">=</span>1.65ms p<span class="o">(</span>90<span class="o">)=</span>0s      p<span class="o">(</span>95<span class="o">)=</span>0s
</span></span><span class="line"><span class="cl">   ✓ http_req_duration..............: <span class="nv">avg</span><span class="o">=</span>13.4ms   <span class="nv">min</span><span class="o">=</span>864µs  <span class="nv">med</span><span class="o">=</span>3.7ms  <span class="nv">max</span><span class="o">=</span>1.96s  p<span class="o">(</span>90<span class="o">)=</span>11.43ms p<span class="o">(</span>95<span class="o">)=</span>43.16ms
</span></span><span class="line"><span class="cl">       <span class="o">{</span> expected_response:true <span class="o">}</span>...: <span class="nv">avg</span><span class="o">=</span>13.4ms   <span class="nv">min</span><span class="o">=</span>864µs  <span class="nv">med</span><span class="o">=</span>3.7ms  <span class="nv">max</span><span class="o">=</span>1.96s  p<span class="o">(</span>90<span class="o">)=</span>11.43ms p<span class="o">(</span>95<span class="o">)=</span>43.16ms
</span></span><span class="line"><span class="cl">     http_req_failed................: 0.00%   ✓ <span class="m">0</span>         ✗ <span class="m">15090</span>
</span></span><span class="line"><span class="cl">     http_req_receiving.............: <span class="nv">avg</span><span class="o">=</span>101.68µs <span class="nv">min</span><span class="o">=</span>10µs   <span class="nv">med</span><span class="o">=</span>79µs   <span class="nv">max</span><span class="o">=</span>5.31ms p<span class="o">(</span>90<span class="o">)=</span>167µs   p<span class="o">(</span>95<span class="o">)=</span>217µs
</span></span><span class="line"><span class="cl">     http_req_sending...............: <span class="nv">avg</span><span class="o">=</span>47.68µs  <span class="nv">min</span><span class="o">=</span>4µs    <span class="nv">med</span><span class="o">=</span>37µs   <span class="nv">max</span><span class="o">=</span>5.87ms p<span class="o">(</span>90<span class="o">)=</span>70µs    p<span class="o">(</span>95<span class="o">)=</span>88µs
</span></span><span class="line"><span class="cl">     http_req_tls_handshaking.......: <span class="nv">avg</span><span class="o">=</span>0s       <span class="nv">min</span><span class="o">=</span>0s     <span class="nv">med</span><span class="o">=</span>0s     <span class="nv">max</span><span class="o">=</span>0s     p<span class="o">(</span>90<span class="o">)=</span>0s      p<span class="o">(</span>95<span class="o">)=</span>0s
</span></span><span class="line"><span class="cl">     http_req_waiting...............: <span class="nv">avg</span><span class="o">=</span>13.25ms  <span class="nv">min</span><span class="o">=</span>803µs  <span class="nv">med</span><span class="o">=</span>3.54ms <span class="nv">max</span><span class="o">=</span>1.96s  p<span class="o">(</span>90<span class="o">)=</span>11.25ms p<span class="o">(</span>95<span class="o">)=</span>43.05ms
</span></span><span class="line"><span class="cl">     http_reqs......................: <span class="m">15090</span>   50.306331/s
</span></span><span class="line"><span class="cl">     iteration_duration.............: <span class="nv">avg</span><span class="o">=</span>13.84ms  <span class="nv">min</span><span class="o">=</span>1.06ms <span class="nv">med</span><span class="o">=</span>4.17ms <span class="nv">max</span><span class="o">=</span>1.96s  p<span class="o">(</span>90<span class="o">)=</span>12.02ms p<span class="o">(</span>95<span class="o">)=</span>43.55ms
</span></span><span class="line"><span class="cl">     iterations.....................: <span class="m">15090</span>   50.306331/s
</span></span><span class="line"><span class="cl">     vus............................: <span class="m">0</span>       <span class="nv">min</span><span class="o">=</span><span class="m">0</span>       <span class="nv">max</span><span class="o">=</span><span class="m">13</span>
</span></span><span class="line"><span class="cl">     vus_max........................: <span class="m">100</span>     <span class="nv">min</span><span class="o">=</span><span class="m">100</span>     <span class="nv">max</span><span class="o">=</span><span class="m">100</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">running <span class="o">(</span>5m00.0s<span class="o">)</span>, 000/100 VUs, <span class="m">15090</span> <span class="nb">complete</span> and <span class="m">0</span> interrupted iterations
</span></span><span class="line"><span class="cl">load ✓ <span class="o">[======================================]</span> 000/100 VUs  5m0s  000.65 iters/s
</span></span></code></pre></div><p>As you can observe, our Kotlin/JVM service has performed very well under heavy load, with a Success Share of 100%, a median latency of ~3ms, and a 95th percentile latency of ~50ms!</p>
<p>Once again, the HPA was able to scale the Deployment from 2 to 8 Pods swiftly and automatically, based on the Pods CPU usage alone!</p>
<blockquote>
<p><em>Note: if you keep the Deployment idle for a few minutes, you should see the HPA gradually scaling back down to 2 Pods, due to low CPU usage.</em></p>
</blockquote>
<h2 id="wrapping-up">Wrapping up</h2>
<hr>
<p>That&rsquo;s it! You can now stop and delete your Kind cluster.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kind delete cluster
</span></span></code></pre></div><p>To summarize, using Metrics Server we were able to:</p>
<ul>
<li>Autoscale horizontally our native service written in Golang, based on both CPU and memory usage;</li>
<li>Autoscale horizontally our JVM service written in Kotlin/JVM, based on CPU usage.</li>
</ul>
<p>Was it worth it? Did that help you understand how to implement Horizontal Pod Autoscaler in Kubernetes using Metrics Server?</p>
<p>If so, follow me on <a href="https://twitter.com/jhandguy">Twitter</a>, I’ll be happy to answer any of your questions and you’ll be the first one to know when a new article comes out! 👌</p>
<p>See you next month, for Part 2 of my series <strong>Horizontal Pod Autoscaler in Kubernetes</strong>!</p>
<p>Bye-bye! 👋</p>

        </div>
    </div>
</section>

</body>
<footer>
    <div class="footer pt-3 has-background-black-bis">
        <div class="container pb-6 has-text-centered is-mobile columns">
            <div class="column is-narrow paddingless">
                
                
                <a class="button is-large" href="https://jhandguy.github.io/posts/incremental-mobile-force-update/"><strong>👈</strong></a>
                
                
            </div>
            <div class="column"></div>
            <div class="column is-narrow paddingless">
                <a class="button is-large" href="#"><strong>👆</strong></a>
            </div>
            <div class="column"></div>
            <div class="column is-narrow paddingless">
                
                
                <a class="button is-large" href="https://jhandguy.github.io/posts/advanced-horizontal-autoscaling/"><strong>👉</strong></a>
                
                
            </div>
        </div>
        <div class="content has-text-centered">
            <p class="subtitle is-6">Powered by <a href="https://gohugo.io/">Hugo</a></p>
            <a href="https://bulma.io">
                <img src="/images/footer/made-with-bulma.png" alt="Made with Bulma" width="128" height="24">
            </a>
        </div>
    </div>
</footer>

</html>
