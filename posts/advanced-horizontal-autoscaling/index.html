<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Horizontal Pod Autoscaler in Kubernetes (Part 2) ‚Äî Advanced Autoscaling using Prometheus Adapter</title>
    <link rel="stylesheet" href="/css/bulma.min.css">
    <link rel="stylesheet" href="/css/custom.css">
</head>
<body>
<nav class="navbar">
    <div class="navbar-brand">
        <a class="navbar-item" href="/">
            Home
        </a>
        <div class="navbar-item">
            <a class="button is-link" href="/about">
                <strong>About me</strong>
            </a>
        </div>
    </div>
</nav>

<section class="section">
    <div class="container is-max-desktop">
        <div class="content">
            <p class="title is-spaced">Horizontal Pod Autoscaler in Kubernetes (Part 2) ‚Äî Advanced Autoscaling using Prometheus Adapter</p>
            
            
            <div class="box">
                <p class="subtitle">Table of Contents</p>
                <nav id="TableOfContents">
  <ul>
    <li><a href="#requirements">Requirements</a></li>
    <li><a href="#creating-kind-cluster">Creating Kind Cluster</a></li>
    <li><a href="#installing-nginx-ingress-controller">Installing NGINX Ingress Controller</a></li>
    <li><a href="#installing-prometheus">Installing Prometheus</a></li>
    <li><a href="#installing-prometheus-adapter">Installing Prometheus Adapter</a></li>
    <li><a href="#autoscaling-services-based-on-prometheus-metrics">Autoscaling Services based on Prometheus Metrics</a></li>
    <li><a href="#wrapping-up">Wrapping up</a></li>
  </ul>
</nav>
            </div>
            
            
        </div>
    </div>
    <div class="container is-max-desktop mt-6">
        <div class="content has-text-justified-tablet">
            <p>The Horizontal Pod Autoscaler (HPA) is a fundamental feature of Kubernetes. It enables automatic scale-up and scale-down of containerized applications based on CPU usage, memory usage, or custom metrics.</p>
<p>Traditionally, when scaling software, we first think of vertical scaling: the CPU and the RAM are increased so the application consuming them can perform better. While this seems like a flawless mechanism on paper, it actually comes with many drawbacks.</p>
<p>First, upgrading the CPU or RAM on a physical machine (or VM) requires downtime and unless a Pod Disruption Budget (PDB) is used to handle <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions">disruptions</a>, all pods will be evicted and recreated in the new resized node.</p>
<p>Nodes resource usage is also not optimized, as scaling vertically means requiring sufficient resources in a single node, while horizontal scaling may have the same amount of resources distributed across multiple nodes.</p>
<p>Additionally, vertical scaling is not as resilient as horizontal scaling, as fewer replicas mean higher risks of disruptions in case of node failure.</p>
<p>Finally, reaching a certain threshold, scaling only vertically becomes very expensive and most importantly, isn‚Äôt limitless. In fact, there is only so much CPU and RAM a physical machine(or VM) alone can handle.<br>
This is where horizontal scaling comes into play!</p>
<blockquote>
<p>Eventually, it is more efficient to duplicate an instance, than increase its resources.</p></blockquote>
<p>üé¨ Hi there, I&rsquo;m Jean!</p>
<p>In this 2 parts series, we&rsquo;re going to explore several ways to scale services horizontally in Kubernetes, and the second one is‚Ä¶<br>
ü•Å<br>
‚Ä¶ using <strong>Prometheus Adapter</strong>! üéä</p>
<h2 id="requirements">Requirements</h2>
<hr>
<p>Before we start, make sure you have the following tools installed:</p>
<ul>
<li><a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installation">Kind</a></li>
<li><a href="https://kubernetes.io/docs/tasks/tools/">Kubectl</a></li>
<li><a href="https://helm.sh/docs/intro/install/">Helm</a></li>
<li><a href="https://grafana.com/docs/k6/latest/set-up/install-k6/">K6</a></li>
</ul>
<blockquote>
<p><em>Note: for MacOS users or Linux users using Homebrew, simply run:</em><br>
<code>brew install kind kubectl helm k6</code></p></blockquote>
<p>All set? Let‚Äôs go! üèÅ</p>
<h2 id="creating-kind-cluster">Creating Kind Cluster</h2>
<hr>
<p><a href="https://kind.sigs.k8s.io/">Kind</a> is a tool for running local Kubernetes clusters using Docker container ‚Äúnodes‚Äù.
It was primarily designed for testing Kubernetes itself, but may be used for local development or CI.</p>
<p>I don‚Äôt expect you to have a demo project in handy, so <a href="https://github.com/jhandguy/canary-deployment">I built one</a> for you.</p>
<pre><code class="language-shell">git clone https://github.com/jhandguy/horizontal-pod-autoscaler.git
cd horizontal-pod-autoscaler
</code></pre>
<p>Alright, let&rsquo;s spin up our Kind cluster! üöÄ</p>
<pre><code class="language-shell">‚ûú kind create cluster --image kindest/node:v1.27.3 --config=kind/cluster.yaml
Creating cluster &quot;kind&quot; ...
 ‚úì Ensuring node image (kindest/node:v1.27.3) üñº
 ‚úì Preparing nodes üì¶
 ‚úì Writing configuration üìú
 ‚úì Starting control-plane üïπÔ∏è
 ‚úì Installing CNI üîå
 ‚úì Installing StorageClass üíæ
Set kubectl context to &quot;kind-kind&quot;
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community üôÇ
</code></pre>
<h2 id="installing-nginx-ingress-controller">Installing NGINX Ingress Controller</h2>
<hr>
<p><a href="https://github.com/kubernetes/ingress-nginx">NGINX Ingress Controller</a> is one of the <a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers">many available Kubernetes Ingress Controllers</a>, which acts as a load balancer and satisfies routing rules specified in <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#what-is-ingress">Ingress</a> resources, using the <a href="https://nginx.org/en/">NGINX reverse proxy</a>.</p>
<p>NGINX Ingress Controller can be installed via its <a href="https://github.com/kubernetes/ingress-nginx/tree/main/charts/ingress-nginx">Helm chart</a>.</p>
<pre><code class="language-shell">helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm install ingress-nginx/ingress-nginx --name-template ingress-nginx --create-namespace -n ingress-nginx --values kind/ingress-nginx-values.yaml --version 4.8.3 --wait
</code></pre>
<p>Now, if everything goes according to plan, you should be able to see the <strong>ingress-nginx-controller</strong> Deployment running.</p>
<pre><code class="language-shell">‚ûú kubectl get deploy -n ingress-nginx
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
ingress-nginx-controller   1/1     1            1           4m35s
</code></pre>
<h2 id="installing-prometheus">Installing Prometheus</h2>
<hr>
<p>Prometheus can be installed via its community <a href="https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack">Helm chart</a>, which also provides <a href="https://grafana.com/grafana/">Grafana</a> out of the box.</p>
<pre><code class="language-shell">helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm install prometheus-community/kube-prometheus-stack --name-template prometheus --create-namespace -n prometheus --version 54.2.2 --wait
</code></pre>
<p>If everything went fine, you should be able to see three newly spawned deployments with the READY state!</p>
<pre><code class="language-shell">‚ûú kubectl get deploy -n prometheus
NAME                                  READY   UP-TO-DATE   AVAILABLE   
prometheus-grafana                    1/1     1            1           
prometheus-kube-prometheus-operator   1/1     1            1           
prometheus-kube-state-metrics         1/1     1            1
</code></pre>
<h2 id="installing-prometheus-adapter">Installing Prometheus Adapter</h2>
<hr>
<p><a href="https://github.com/kubernetes-sigs/prometheus-adapter">Prometheus Adapter</a> is a source of custom metrics, which collects them from Prometheus and exposes them in Kubernetes API server through <a href="https://github.com/kubernetes/metrics">Metrics API</a> for use by <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler</a> and <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler/">Vertical Pod Autoscaler</a>.</p>
<p>Unlike <a href="https://github.com/kubernetes-sigs/metrics-server">Metrics Server</a> which is limited to resource metrics, Prometheus Adapter exposes custom metrics measurable from within a Pod&rsquo;s container, such as memory usage, GC duration, but also request throughput, latency, etc.</p>
<blockquote>
<p><em>If you haven&rsquo;t already, go read <a href="/posts/simple-horizontal-autoscaling/">Horizontal Pod Autoscaler in Kubernetes (Part 1) - Simple Autoscaling using Metrics Server</a> and learn how to implement a Horizontal Pod Autoscaler using Metrics Server!</em></p></blockquote>
<p>Prometheus Adapter can be installed via its <a href="https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-adapter">Helm chart</a>.</p>
<pre><code class="language-shell">helm install prometheus-community/prometheus-adapter --name-template prometheus-adapter --create-namespace -n prometheus-adapter --values kind/prometheus-adapter-values.yaml --version 4.9.0 --wait
</code></pre>
<p>Now, if all goes well, you should see the <strong>prometheus-adapter</strong> Deployment running with the READY state.</p>
<pre><code class="language-shell">‚ûú kubectl get deploy -n prometheus-adapter
NAME                 READY   UP-TO-DATE   AVAILABLE   AGE
prometheus-adapter   1/1     1            1           52s
</code></pre>
<h2 id="autoscaling-services-based-on-prometheus-metrics">Autoscaling Services based on Prometheus Metrics</h2>
<hr>
<p>Now that Prometheus Adapter is up and running, let&rsquo;s get to it, shall we? üßê</p>
<pre><code class="language-shell">helm install golang-sample-app/helm-chart --name-template sample-app --create-namespace -n sample-app --set prometheus.enabled=true --wait
</code></pre>
<p>If everything goes fine, you should eventually see one Deployment with the READY state.</p>
<pre><code class="language-shell">‚ûú kubectl get deploy -n sample-app
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
sample-app   2/2     2            2           28s
</code></pre>
<p>Alright, now let&rsquo;s have a look at the HPA!</p>
<pre><code class="language-shell">‚ûú kubectl describe hpa -n sample-app
...
Metrics: ( current / target )
  &quot;golang_sample_app_requests_per_second&quot; on pods:  &lt;unknown&gt; / 10
Min replicas:                                       2
Max replicas:                                       8
</code></pre>
<p>As you can see, this HPA is configured to scale the service based on requests per second (rps), with an average of <strong>10</strong>.</p>
<blockquote>
<p><em>Note: as you probably have noticed, the current value for the request throughput is <strong>unknown</strong>, this is expected as the service hasn&rsquo;t served any requests yet, thus no timeseries for this metric are present in Prometheus.</em></p></blockquote>
<p>This means that as soon as the request throughput breaches the <strong>10rps</strong> threshold, the HPA will trigger an upscale.</p>
<p>Under minimal load, the HPA will still retain a replica count of <strong>2</strong>, while the maximum amount of Pods the HPA is allowed to spin up under high load is <strong>8</strong>.</p>
<blockquote>
<p><em>Note: in a production environment, it is recommended to have a minimum replica count of at least 3, to guarantee maintained availability in the case of <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/_print/#pod-disruption">Pod Disruption</a>.</em></p></blockquote>
<p>Now, this is the moment you&rsquo;ve certainly expected‚Ä¶ It&rsquo;s Load Testing time! üòé</p>
<p>For Load Testing, I really recommend <a href="https://grafana.com/docs/k6/latest/">k6</a> from the Grafana Labs team. It is a dead-simple yet super powerful tool with very extensive documentation.</p>
<p>See for yourself!</p>
<pre><code class="language-shell">k6 run k6/script.js
</code></pre>
<p>While the load test is running, I suggest watching the HPA in a separate tab.</p>
<pre><code class="language-shell">kubectl get hpa -n sample-app -w
</code></pre>
<p>As the load test progresses and the 2 starting Pods start to handle more than 10rps each, you should see the Prometheus metric&rsquo;s target increasing, and ultimately, the <strong>replica count reaching its maximum</strong>!</p>
<pre><code class="language-shell">Deployment/sample-app   &lt;unknown&gt;/10   2         8         2
Deployment/sample-app   4360m/10       2         8         2
Deployment/sample-app   6236m/10       2         8         2
Deployment/sample-app   12471m/10      2         8         2
Deployment/sample-app   15577m/10      2         8         3
Deployment/sample-app   21231m/10      2         8         3
Deployment/sample-app   21231m/10      2         8         5
Deployment/sample-app   16752m/10      2         8         5
Deployment/sample-app   18362m/10      2         8         5
Deployment/sample-app   15921m/10      2         8         6
Deployment/sample-app   15543m/10      2         8         6
Deployment/sample-app   16530m/10      2         8         8
Deployment/sample-app   15397m/10      2         8         8
Deployment/sample-app   14428m/10      2         8         8
Deployment/sample-app   11897m/10      2         8         8
Deployment/sample-app   12370m/10      2         8         8
Deployment/sample-app   12423m/10      2         8         8
Deployment/sample-app   12414m/10      2         8         8
Deployment/sample-app   12423m/10      2         8         8
Deployment/sample-app   10962m/10      2         8         8
</code></pre>
<p>Now, let&rsquo;s quickly have a look at the Load Test summary and the result of the <code>http_req_duration</code> metric in particular!</p>
<pre><code class="language-shell">          /\      |‚Äæ‚Äæ| /‚Äæ‚Äæ/   /‚Äæ‚Äæ/
     /\  /  \     |  |/  /   /  /
    /  \/    \    |     (   /   ‚Äæ‚Äæ\
   /          \   |  |\  \ |  (‚Äæ)  |
  / __________ \  |__| \__\ \_____/ .io

  execution: local
     script: k6/script.js
     output: -

  scenarios: (100.00%) 1 scenario, 100 max VUs, 5m30s max duration (incl. graceful stop):
           * load: Up to 100.00 iterations/s for 5m0s over 2 stages (maxVUs: 100, gracefulStop: 30s)


     ‚úì status code is 200
     ‚úì node is kind-control-plane
     ‚úì namespace is sample-app
     ‚úì pod is sample-app-*

   ‚úì checks.........................: 100.00% ‚úì 60356     ‚úó 0
     data_received..................: 3.5 MB  12 kB/s
     data_sent......................: 1.7 MB  5.8 kB/s
     http_req_blocked...............: avg=18.43¬µs min=1¬µs     med=8¬µs    max=3.9ms  p(90)=17¬µs    p(95)=20¬µs
     http_req_connecting............: avg=5.41¬µs  min=0s      med=0s     max=3.7ms  p(90)=0s      p(95)=0s
   ‚úì http_req_duration..............: avg=16.74ms min=498¬µs   med=2.77ms max=1.48s  p(90)=14.52ms p(95)=64.78ms
       { expected_response:true }...: avg=16.74ms min=498¬µs   med=2.77ms max=1.48s  p(90)=14.52ms p(95)=64.78ms
     http_req_failed................: 0.00%   ‚úì 0         ‚úó 15089
     http_req_receiving.............: avg=97.15¬µs min=9¬µs     med=74¬µs   max=3.33ms p(90)=151¬µs   p(95)=197¬µs
     http_req_sending...............: avg=48.11¬µs min=6¬µs     med=34¬µs   max=3.12ms p(90)=69¬µs    p(95)=86¬µs
     http_req_tls_handshaking.......: avg=0s      min=0s      med=0s     max=0s     p(90)=0s      p(95)=0s
     http_req_waiting...............: avg=16.59ms min=452¬µs   med=2.61ms max=1.48s  p(90)=14.32ms p(95)=64.69ms
     http_reqs......................: 15089   50.297179/s
     iteration_duration.............: avg=17.18ms min=610.5¬µs med=3.21ms max=1.48s  p(90)=15.08ms p(95)=65.33ms
     iterations.....................: 15089   50.297179/s
     vus............................: 0       min=0       max=18
     vus_max........................: 100     min=100     max=100


running (5m00.0s), 000/100 VUs, 15089 complete and 0 interrupted iterations
load ‚úì [======================================] 000/100 VUs  5m0s  000.65 iters/s
</code></pre>
<p>As you can observe, our service has performed very well under heavy load, with a Success Share of 100%, a median latency of ~3ms, and a 95th percentile latency of ~50ms!</p>
<p>We have the HPA to thank for that, as it scaled the Deployment from 2 to 8 Pods swiftly and automatically, based on request throughput!</p>
<p>We definitely would not have had the same results without an HPA‚Ä¶ Actually, why don&rsquo;t you try it yourself? üòâ</p>
<p>Just delete the HPA (<code>kubectl delete hpa sample-app -n sample-app</code>), run the load test again (<code>k6 run k6/script.js</code>) and see what happens! (spoiler alert: it&rsquo;s not pretty üò¨)</p>
<h2 id="wrapping-up">Wrapping up</h2>
<hr>
<p>That&rsquo;s it! You can now stop and delete your Kind cluster.</p>
<pre><code class="language-shell">kind delete cluster
</code></pre>
<p>To summarize, using Prometheus Adapter we were able to:</p>
<ul>
<li>Autoscale horizontally our service, based on Prometheus metrics.</li>
</ul>
<p>Was it worth it? Did that help you understand how to implement Horizontal Pod Autoscaler in Kubernetes using Prometheus Adapter?</p>
<p>If so, follow me on <a href="https://x.com/jhandguy">X</a>, I‚Äôll be happy to answer any of your questions and you‚Äôll be the first one to know when a new article comes out! üëå</p>
<p>Bye-bye! üëã</p>

        </div>
    </div>
</section>

</body>
<footer>
    <div class="footer pt-3 has-background-black-bis">
        <div class="container pb-6 has-text-centered is-mobile columns">
            <div class="column is-narrow paddingless">
                
                
                <a class="button is-large" href="https://jhandguy.github.io/posts/simple-horizontal-autoscaling/"><strong>üëà</strong></a>
                
                
            </div>
            <div class="column"></div>
            <div class="column is-narrow paddingless">
                <a class="button is-large" href="#"><strong>üëÜ</strong></a>
            </div>
            <div class="column"></div>
            <div class="column is-narrow paddingless">
                
                
                <a class="button is-large" href="https://jhandguy.github.io/posts/vertical-pod-autoscaler/"><strong>üëâ</strong></a>
                
                
            </div>
        </div>
        <div class="content has-text-centered">
            <p class="subtitle is-6">Powered by <a href="https://gohugo.io/">Hugo</a></p>
            <a href="https://bulma.io">
                <img src="/images/footer/made-with-bulma.png" alt="Made with Bulma" width="128" height="24">
            </a>
        </div>
    </div>
</footer>

</html>
