<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Farewell, ECS! Hello, EKS: A Kubernetes Migration Story</title>
    <link rel="stylesheet" href="/css/bulma.min.css">
    <link rel="stylesheet" href="/css/custom.css">
    <link rel="stylesheet" href="/css/hugo.css">
</head>

<body>
<nav class="navbar">
    <div class="navbar-brand">
        <a class="navbar-item" href="/">
            Home
        </a>

        <div class="navbar-item">
            <a class="button is-link" href="/about">
                <strong>About me</strong>
            </a>
        </div>
    </div>
</nav>

<section class="section">
    <div class="container is-max-desktop">
        <div class="content">
            <p class="title is-spaced">Farewell, ECS! Hello, EKS: A Kubernetes Migration Story</p>
            
            
            <div class="box">
                <p class="subtitle">Table of Contents</p>
                <nav id="TableOfContents">
  <ul>
    <li><a href="#starting-with-the-why">Starting with the Why</a></li>
    <li><a href="#evaluating-feasibility-with-a-proof-of-concept-poc">Evaluating Feasibility with a Proof of Concept (PoC)</a></li>
    <li><a href="#testing-staging-workloads-in-eks">Testing Staging Workloads in EKS</a></li>
    <li><a href="#deploying-production-workloads-to-eks">Deploying Production Workloads to EKS</a></li>
    <li><a href="#enabling-asynchronous-processing-in-eks">Enabling Asynchronous Processing in EKS</a></li>
    <li><a href="#routing-user-traffic-gradually-to-eks">Routing User Traffic Gradually to EKS</a></li>
    <li><a href="#scaling-down-workloads-in-ecs">Scaling down Workloads in ECS</a></li>
    <li><a href="#acknowledgements">Acknowledgements</a></li>
  </ul>
</nav>
            </div>
            
            
        </div>
    </div>
    <div class="container is-max-desktop mt-6">
        <div class="content has-text-justified-tablet">
            <p>Saying goodbye to old systems can sound scary, but for us at EGYM, migrating from Elastic Container Service (ECS) to Elastic Kubernetes Service (EKS) was a journey of growth and excitement.</p>
<p>üé¨ Hi there, I‚Äôm Jean!</p>
<p>In this blog post, I‚Äôll share the inside scoop on our successful migration from ECS to EKS, why we did it and how, complete with the magic formula for <strong>zero downtime</strong>!</p>
<h2 id="starting-with-the-why">Starting with the Why</h2>
<hr>
<p>Let‚Äôs kick it off with a famous phrase in engineering:</p>
<blockquote>
<p>If it ain‚Äôt broke, don‚Äôt fix it</p>
</blockquote>
<p>Although this holds true in most cases, there are some situations where the costs of keeping the lights on for ‚Äúwhat isn‚Äôt broken‚Äù, can no longer be justified.</p>
<p>Originally, EGYM‚Äôs entire infrastructure was hosted on a single Cloud, Google Cloud Platform (GCP), and all of our workloads were running in Google Kubernetes Engine (GKE). This held true until EGYM acquired a company, which made us embrace multi-Cloud by adding 3 ECS clusters hosted on Amazon Web Services (AWS).</p>
<p>While the merger itself was more or less successful from a business perspective (depending who you ask), maintaining, coaching and hiring engineers with the necessary skills to work with 2 Clouds, fundamentally different yet doing pretty much the same thing, became challenging and most importantly: costly.</p>
<p>With Kubernetes being Cloud-agnostic by design, we soon realized there could be potential for porting most of the core components we already used in GKE, to EKS. This fact alone made the case for moving away from ECS towards EKS much easier to sell to upper management.</p>
<p>Very soon, we all unanimously agreed in the SRE team that migrating to EKS was the way forward, and that we needed to evaluate its feasibility.</p>
<p><img src="/images/farewell-ecs-hello-eks/1.png" alt="Kubernetes, Kubernetes everywhere"></p>
<h2 id="evaluating-feasibility-with-a-proof-of-concept-poc">Evaluating Feasibility with a Proof of Concept (PoC)</h2>
<hr>
<p>One of the main goals of this PoC, was to design an EKS cluster from the ground up with all the core infrastructure components that our micro-services would need to serve traffic reliably (load balancing, auto-scaling, monitoring, etc).</p>
<p>From all the technical choices we had to make, deciding which Load Balancer to pick was by far the most challenging. <a href="https://github.com/kubernetes/ingress-nginx">Ingress NGINX</a> seemed like an obvious candidate for its Cloud-agnostic design, yet <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller">AWS Load Balancer Controller</a> also caught our eye for its native integration with AWS Load Balancers.</p>
<p>AWS Load Balancer Controller is not a Load Balancer per se, but the Controller that provisions them. One can either provision a Network Load Balancer (NLB) using the <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.7/guide/service/annotations">Service annotations</a> or an Application Load Balancer (ALB) using the <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.7/guide/ingress/annotations">Ingress annotations</a>.</p>
<p>Unless explicitly configured otherwise, AWS Load Balancer Controller will provision an AWS Load Balancer per Service/Ingress resource, which can be a driver for higher costs.</p>
<p><img src="/images/farewell-ecs-hello-eks/2.png" alt="Diagram of AWS Load Balancer Controller in EKS"></p>
<p>Ingress NGINX is fundamentally different in the fact that it does not only act as a Controller, but also as an L7 Load Balancer: it is responsible for both satisfying Ingress rules and load balancing HTTP traffic to the corresponding Services.</p>
<p>While AWS Load Balancer Controller can be used to provision both ALBs (L7 Load Balancers) and NLBs (L4 Load Balancers), Ingress NGINX needs to be itself load balanced by an L4 Load Balancer (NLB), before taking over and load balance traffic on the application layer.</p>
<p><img src="/images/farewell-ecs-hello-eks/3.png" alt="Diagram of NGINX Ingress Controller in EKS"></p>
<h2 id="testing-staging-workloads-in-eks">Testing Staging Workloads in EKS</h2>
<hr>
<p>As soon as our EKS clusters architecture was designed and showcased in a PoC, it was time to port the staging workloads to a new shiny EKS test cluster and make sure they function equally well in ECS and EKS.</p>
<p>While this part was mostly smooth sailing, here are a few issues that we faced along the way:</p>
<ul>
<li>Many of our micro-services hosted on AWS are Java services, which are using the <a href="https://aws.amazon.com/sdk-for-java">AWS SDK for Java</a> to interact with AWS services. On paper this should not be a problem however, the authentication method used in EKS (<a href="https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html">AssumeRoleWithWebIdentity</a>) is different than in ECS (<a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/instance_IAM_role.html">AmazonEC2ContainerServiceforEC2Role</a>) and sometimes, based on how the SDK was utilized or whether the AWS STS package was found in the classpath, the default authentication chain provider would choose the wrong authentication method in EKS and fail to authenticate.</li>
<li>Some of our micro-services perform asynchronous processing (also known as event messaging), which meant we had to ensure each event consumption was done in an idempotent operation or else be at risk of data duplication/corruption during the migration.</li>
</ul>
<p>Once the staging workloads were successfully deployed and tested, it was time to migrate the production workloads to EKS as well!</p>
<h2 id="deploying-production-workloads-to-eks">Deploying Production Workloads to EKS</h2>
<hr>
<p>To ensure maximum availability during the migration, we decided to run all our workloads twice: one deployment remained in the existing ECS production cluster and a new replica set was deployed in the new EKS production cluster.</p>
<p>This approach had the following advantages:</p>
<ul>
<li><strong>Isolated Testing</strong>: Since our ECS cluster was already serving user traffic, it was easy to deploy and test all production services via a separate domain and Ingress in the EKS cluster without affecting users.</li>
<li><strong>Blue-Green Deployment</strong>: By far the biggest advantage of running the EKS cluster along side the ECS one, was the ability to route user traffic to EKS gradually and to quickly roll it back to ECS in case things went south.</li>
</ul>
<p>However, running things twice also had some drawbacks:</p>
<ul>
<li><strong>Added Costs</strong>: Duplicating the replica sets also meant doubling the costs for a short period of time, which was a conscious investment we made to guarantee high availability during the entirety of the migration.</li>
<li><strong>Increased Connections</strong>: With twice more workloads, twice more connections were opened to our data infrastructure such as MySQL databases, Redis instances and ActiveMQ brokers, which meant raising the connection limit temporarily in some cases.</li>
</ul>
<p>Eventually, all EGYM‚Äôs production workloads were deployed to EKS and tested successfully, in isolation from the services deployed in ECS.</p>
<p>The next step was to execute our rollout plan, which consisted of 3 phases:</p>
<ul>
<li><strong>Enabling Asynchronous Processing in EKS</strong></li>
<li><strong>Routing User Traffic Gradually to EKS</strong></li>
<li><strong>Scaling down Workloads in ECS</strong></li>
</ul>
<h2 id="enabling-asynchronous-processing-in-eks">Enabling Asynchronous Processing in EKS</h2>
<hr>
<p>While deploying EGYM‚Äôs micro-services to EKS, we took the precautionary measure of disabling asynchronous processing (PubSub, SQS/SNS, etc.) for all workloads deployed in EKS.</p>
<p>This way, we ensured all upstream connections were ready before we would enable event processing gradually, giving us plenty of time to observe and ensure events were processed properly.</p>
<p>Once asynchronous processing was enabled on all replica sets in EKS, it was finally time for the most exciting yet also treacherous part of the migration: routing user traffic.</p>
<h2 id="routing-user-traffic-gradually-to-eks">Routing User Traffic Gradually to EKS</h2>
<hr>
<p>In order to gradually pour traffic in the new EKS cluster, we took advantage of 2 routing mechanisms:</p>
<ul>
<li><strong><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/continuous-deployment.html">CloudFront Staging Distribution</a></strong><br>
With most of our AWS traffic being served on CloudFront‚Äôs edges, one easy and powerful way of routing traffic to EKS gradually, was using CloudFront Staging Distribution.
Simply put, a Staging Distribution is like a blue-green deployment but for Content Delivery Networks (CDN): when deploying a new proxy configuration, one can choose to deploy it only to a subset of users and roll it out gradually to observe the change while minimizing risks.
The advantage of a Staging Distribution is that it is directly deployed on the edges, meaning that rolling back in case of an incident is almost instant.
The drawback on the other hand, is that it is currently only possible to route up to 15% of user traffic to a Staging Distribution before promoting it to a Production Distribution.</li>
<li><strong><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-weighted.html">Route 53 Weighted Routing</a></strong><br>
This is where Route 53 Weighted Routing comes in: for routing more than 15% of user traffic to EKS, we had to temporarily route user traffic away from the CDN, and directly towards the NLB.
With Weighted Routing, one has complete control of how much traffic is sent to either one of the origins however, the Time To Live (TTL) of the record (60 seconds for A records) must be taken into account when rolling back with DNS.</li>
</ul>
<p>By exploiting both <strong>CloudFront Staging Distribution</strong> and <strong>Route 53 Weighted Routing</strong> routing mechanisms, we were able to gradually transfer traffic from ECS to EKS while benefiting from the incredible rollback speed of CloudFront for the first 15% of routed traffic, thus ensuring the highest availability possible.</p>
<p><img src="/images/farewell-ecs-hello-eks/4.png" alt="Example of 50/50 traffic split using Route 53 Weighted Routing and CloudFront Staging Distribution"></p>
<p>We proceeded with a gradual rollout of roughly 2 weeks, starting at 1%, followed by 5%, 10%, 15%, 25%, 35%, 50%, 65%, 80% and finally 100%.</p>
<p><img src="/images/farewell-ecs-hello-eks/5.png" alt="NGINX Traffic Distribution between ECS and EKS during migration"></p>
<p>For the first 15%, traffic routed to EKS was entirely served on CloudFront‚Äôs edges, and above the 15%, weighted routing was handled on DNS level via Route 53.</p>
<p>Once arrived at 80% of traffic routed to EKS, the last bump to 100% was achieved, not by routing traffic exclusively from Route 53, but by promoting the CloudFront Staging Distribution to Production Distribution instead and transfer all user traffic back to CloudFront.</p>
<p><img src="/images/farewell-ecs-hello-eks/6.png" alt="CloudFront Request Throughput and Latency during migration"></p>
<p>As more traffic entered the EKS cluster, we were able to cautiously observe the Horizontal Pod Autoscalers and ensure all deployments were scaling properly as the load increased.</p>
<p><img src="/images/farewell-ecs-hello-eks/7.png" alt="Ingress NGINX Request Success Share and Average Latency during migration"></p>
<p>Ultimately, with the EKS cluster reporting stable and the availability of the services left intact, we were ready to proceed with the ECS scale-down.</p>
<h2 id="scaling-down-workloads-in-ecs">Scaling down Workloads in ECS</h2>
<hr>
<p>Sunsetting workloads in ECS was the final moment of truth, leading to the fateful question: did we miss anything?</p>
<p>As we scaled down each service in ECS one by one, we paid special attention to asynchronous processing and event messaging until‚Ä¶</p>
<blockquote>
<p><em>‚ÄúThe last ECS Task, a digital phoenix battling resource constraints, choked on its final heartbeat, its containerized spirit evaporating into the cloud.‚Äù</em><br>
Gemini, February 2024</p>
</blockquote>
<p><img src="/images/farewell-ecs-hello-eks/8.png" alt="Empty Production ECS Cluster"></p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>I would like to thank Kolawole for helping me in carrying this project to its completion, Marty for trusting me in leading the project, as well as Viktor, Serhii, Roma and Eugene for their continued support throughout the migration.</p>
<hr>
<p>That‚Äôs it! I hope this experience was valuable to you.</p>
<p>If so, follow me on <a href="https://twitter.com/jhandguy">Twitter</a>, I‚Äôll be happy to answer any of your questions and you‚Äôll be the first one to know when a new article comes out! üëå</p>
<p>Bye-bye! üëã</p>

        </div>
    </div>
</section>

</body>
<footer>
    <div class="footer pt-3 has-background-black-bis">
        <div class="container pb-6 has-text-centered is-mobile columns">
            <div class="column is-narrow paddingless">
                
                
                <a class="button is-large" href="https://jhandguy.github.io/posts/vertical-pod-autoscaler/"><strong>üëà</strong></a>
                
                
            </div>
            <div class="column"></div>
            <div class="column is-narrow paddingless">
                <a class="button is-large" href="#"><strong>üëÜ</strong></a>
            </div>
            <div class="column"></div>
            <div class="column is-narrow paddingless">
                
                
                <a class="button is-large" disabled><strong>üëâ</strong></a>
                
                
            </div>
        </div>
        <div class="content has-text-centered">
            <p class="subtitle is-6">Powered by <a href="https://gohugo.io/">Hugo</a></p>
            <a href="https://bulma.io">
                <img src="/images/footer/made-with-bulma.png" alt="Made with Bulma" width="128" height="24">
            </a>
        </div>
    </div>
</footer>

</html>
